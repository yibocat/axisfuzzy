{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Demonstration Steps:\n",
    "1. Define a sub-Pipeline for performing data preprocessing and statistical calculations.\n",
    "\n",
    "2. Define a main Pipeline.\n",
    "\n",
    "3. Within the main Pipeline, add the sub-Pipeline as a regular step to achieve nesting.\n",
    "\n",
    "4. Demonstrate how to extract the desired results from the output of the nested Pipeline.\n",
    "\n",
    "### Expected output:\n",
    "1. First, the information of the sub-pipeline is created.\n",
    "\n",
    "2. After the hot probe, the main pipeline adds the sub-pipeline as a step.\n",
    "\n",
    "3. At runtime, main_pipeline.run() will trigger execution of the sub-pipeline.\n",
    "\n",
    "4. final_results will contain the final output of the main pipeline. Since we have added only the sub-pipeline step, and the sub-pipeline has multiple terminal outputs, final_results will be a dictionary containing one key, which is the name of the sub-pipeline followed by _result (e.g., PreprocessingAndStats_result), and its value will be a collection (a dictionary) of all terminal outputs within the sub-pipeline.\n",
    "\n",
    "5. intermediate_states will provide a more detailed view of the execution process. You will see the steps within the main pipeline (i.e., the nested sub-pipeline step), as well as results from each step inside the sub-pipeline (normalization, statistics, aggregation)."
   ],
   "id": "9091df28dd2252e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from axisfuzzy.analysis.pipeline import FuzzyPipeline\n",
    "from axisfuzzy.analysis.components.basic import (\n",
    "    NormalizationTool,\n",
    "    StatisticsTool,\n",
    "    SimpleAggregationTool\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_preprocessing_sub_pipeline() -> FuzzyPipeline:\n",
    "    \"\"\"\n",
    "    Creates a sub-pipeline for data preprocessing and basic statistics.\n",
    "    This sub-pipeline will be nested within a larger pipeline.\n",
    "    \"\"\"\n",
    "    sub_pipeline = FuzzyPipeline(name=\"PreprocessingAndStats\")\n",
    "\n",
    "    # Define the input for the sub-pipeline\n",
    "    # 子管道的输入是一个名为 'input_data' 的 CrispTable\n",
    "    input_data = sub_pipeline.input(\"input_data\", contract='CrispTable')\n",
    "\n",
    "    # Step 1: Normalize the data (Min-Max, column-wise)\n",
    "    # 使用 NormalizationTool 对输入数据进行列向的 Min-Max 归一化\n",
    "    normalizer = NormalizationTool(method='min_max', axis=0)\n",
    "    normalized_data = sub_pipeline.add(normalizer.run, data=input_data)\n",
    "\n",
    "    # Step 2: Calculate statistics on the normalized data\n",
    "    # 使用 StatisticsTool 计算归一化后数据的整体统计信息\n",
    "    stats_calculator = StatisticsTool(axis=0) # axis=0 for column-wise stats\n",
    "    statistics_output = sub_pipeline.add(stats_calculator.run, data=normalized_data)\n",
    "\n",
    "    # Step 3: Aggregate the normalized data (e.g., calculate mean for each row)\n",
    "    # 使用 SimpleAggregationTool 计算每行的平均值\n",
    "    aggregator = SimpleAggregationTool(operation='mean', axis=1)\n",
    "    aggregated_values = sub_pipeline.add(aggregator.run, data=normalized_data)\n",
    "\n",
    "    # 注意：这个子管道有两个“末端”输出：statistics_output 和 aggregated_values\n",
    "    # 当它被嵌套时，其输出将是一个字典，包含这两个结果。\n",
    "    return sub_pipeline"
   ],
   "id": "37c22911848d66ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Starting Nested Pipeline Example ---\")\n",
    "# 1. Prepare some sample crisp data\n",
    "data = pd.DataFrame({\n",
    "    'Feature_X': [10, 20, 15, 25, 30],\n",
    "    'Feature_Y': [100, 80, 120, 90, 110],\n",
    "    'Feature_Z': [5, 8, 6, 7, 9]\n",
    "}, index=['Sample_1', 'Sample_2', 'Sample_3', 'Sample_4', 'Sample_5'])\n",
    "print(\"\\nOriginal Data:\")\n",
    "print(data)"
   ],
   "id": "b7412c6eb0406db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Create the sub-pipeline\n",
    "preprocessing_sub_pipeline = create_preprocessing_sub_pipeline()\n",
    "print(f\"\\nCreated Sub-Pipeline: {preprocessing_sub_pipeline}\")"
   ],
   "id": "e2f7d79c3597dd4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Build the main pipeline\n",
    "main_pipeline = FuzzyPipeline(name=\"MainAnalysisFlow\")\n",
    "\n",
    "# Define the input for the main pipeline\n",
    "# 主管道的输入是一个名为 'main_input_data' 的 CrispTable\n",
    "main_input_data = main_pipeline.input(\"main_input_data\", contract='CrispTable')"
   ],
   "id": "380ef261d435b363",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add the sub-pipeline as a step in the main pipeline!\n",
    "# 这是实现嵌套的关键一步。我们将子管道作为一个可调用的工具传递给 add 方法。\n",
    "# 注意：kwargs 的键 ('input_data') 必须匹配子管道的输入名称。\n",
    "sub_pipeline_results = main_pipeline.add(\n",
    "    preprocessing_sub_pipeline,\n",
    "    input_data=main_input_data # 将主管道的输入连接到子管道的输入\n",
    ")"
   ],
   "id": "64db156f57554908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now, `sub_pipeline_results` holds the outputs from the sub-pipeline.\n",
    "# 根据我们对 pipeline.py 的修改，如果子管道有多个末端输出，\n",
    "# `sub_pipeline_results` 将是一个字典，其键是子管道中末端步骤的 display_name。\n",
    "# 我们可以通过这些键来访问子管道的各个输出。\n",
    "# 让我们打印 sub_pipeline_results 的类型和内容来确认\n",
    "print(f\"\\nType of sub_pipeline_results: {type(sub_pipeline_results)}\")\n",
    "# print(f\"Keys available from sub_pipeline_results: {list(sub_pipeline_results.keys())}\")"
   ],
   "id": "c8c23c6c54e12050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 假设我们想对子管道的聚合结果进行进一步处理\n",
    "# 我们可以从 sub_pipeline_results 中提取 'SimpleAggregationTool_run_...' 的输出\n",
    "# 实际的键会是 'SimpleAggregationTool_run_xxxxxxxx'，其中 xxxxxxxx 是随机ID\n",
    "# 为了示例的通用性，我们假设它返回一个名为 'aggregated_values' 的键，或者我们知道其完整的显示名称\n",
    "# 在实际使用中，你可能需要检查 sub_pipeline_results.keys() 来获取确切的键名\n",
    "# 或者，如果子管道只有一个输出，它会直接返回该输出。\n",
    "# 由于我们有两个末端节点，它会返回一个字典。\n",
    "# 让我们假设我们知道键的格式，或者通过打印 sub_pipeline_results.keys() 来获取\n",
    "\n",
    "# 提取子管道的聚合结果\n",
    "# 假设 'SimpleAggregationTool_run_...' 是一个键\n",
    "# 实际键名会是 'SimpleAggregationTool_run_xxxxxxxx'\n",
    "# 为了演示，我们先假设其 display_name 是 'SimpleAggregationTool.run'\n",
    "# 并且其输出是 'aggregated_values'\n",
    "# 那么组合后的键名可能是 'SimpleAggregationTool.run_aggregated_values'\n",
    "# 或者更简单，如果 get_output_contracts 逻辑将它们扁平化，则直接是 'aggregated_values'\n",
    "\n",
    "# 根据之前对 pipeline.py 的修改，如果子管道有多个末端输出，\n",
    "# 并且这些输出被合并到一个字典中，那么键名会是 `display_name_output_name` 的形式。\n",
    "# 比如：'NormalizationTool_run_normalized_data', 'StatisticsTool_run_statistics', 'SimpleAggregationTool_run_aggregated_values'\n",
    "# 并且，如果最终输出多于一个，整个子管道的输出会被包装成一个 'result': 'PipelineResult' 的字典。\n",
    "# 这意味着 sub_pipeline_results 将是一个 StepOutput，代表一个 PipelineResult。\n",
    "# 它的实际值会在运行时被解析。\n",
    "\n",
    "# 为了简化，我们直接运行主管道，并查看最终结果\n",
    "# 最终结果将包含子管道的所有末端输出\n",
    "final_results, intermediate_states = main_pipeline.run(\n",
    "    initial_data={\"main_input_data\": data},\n",
    "    return_intermediate=True\n",
    ")\n",
    "print(\"\\n--- Main Pipeline Execution Results ---\")"
   ],
   "id": "1d04b5e4a9ef7e34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nFinal Outputs from Main Pipeline (includes sub-pipeline's terminal outputs):\")\n",
    "# final_results 将是一个字典，其中包含子管道的末端输出\n",
    "# 键名会是子管道中步骤的 display_name，例如 'PreprocessingAndStats_result'\n",
    "# 然后 'PreprocessingAndStats_result' 的值又是一个字典，包含子管道的实际输出\n",
    "# 比如：{'StatisticsTool.run_statistics': {...}, 'SimpleAggregationTool.run_aggregated_values': {...}}\n",
    "\n",
    "# 让我们直接打印 final_results 来观察其结构\n",
    "print(final_results)\n",
    "\n",
    "print(\"\\n--- All Intermediate States (including nested pipeline's internal results) ---\")\n",
    "# 遍历所有中间状态，可以观察到子管道内部的执行步骤\n",
    "for step_id, result_data in intermediate_states.items():\n",
    "    step_info = main_pipeline.get_step_info(step_id)\n",
    "    print(f\"\\nStep '{step_info['display_name']}' (ID: {step_id[:8]}...):\")\n",
    "    # 对于嵌套管道步骤，其结果是子管道的最终输出\n",
    "    if step_info['display_name'] == preprocessing_sub_pipeline.name:\n",
    "        print(f\"  (This is the nested pipeline step. Its result is the sub-pipeline's final output.)\")\n",
    "    print(result_data)\n",
    "\n",
    "print(\"\\n--- Nested Pipeline Example Finished ---\")"
   ],
   "id": "dcff64164e764163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b9c6d69140f8f0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bf9a3dc80f887431",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
